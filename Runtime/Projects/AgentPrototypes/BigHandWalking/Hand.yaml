behaviors:
  Hand:
    trainer_type: ppo
    hyperparameters:
#batch_size corresponds to minibatch size in ppo.py
      batch_size: 2048
#args.num_steps * args.num_envs | the total number of experiences collected before updating the policy | num_steps in ppo.py
      buffer_size: 81920
      learning_rate: 0.003
      beta: 0.005
      epsilon: 0.3
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
      beta_schedule: linear
      epsilon_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple
      memory: null
      goal_conditioning_type: hyper
      deterministic: false
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
    init_path: null
    keep_checkpoints: 2
    even_checkpoints: true
#    checkpoint_interval: 250000
    max_steps: 50000000
#also like num_steps in ppo.py, if limit reached before episode ends, then value estimation also happens at this point
    time_horizon: 1000
    summary_freq: 100000
    threaded: false
env_settings:
 base_port: 5015
 seed: 42